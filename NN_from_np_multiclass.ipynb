{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (150, 4)\n",
      "y_train: (150, 1)\n",
      "Classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# First lets load some data to run through our model\n",
    "iris = load_iris()\n",
    "X_train = iris['data']\n",
    "y_train = iris['target'].reshape(-1,1)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"Classes:\", np.unique(y_train))\n",
    "y_train_original = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Y_transform(Y):\n",
    "    m = Y.shape[0]\n",
    "    Y_unique = np.unique(Y)\n",
    "    Y_trans = []\n",
    "    for i in range(0,3):\n",
    "        yarray = np.multiply(Y==Y_unique[i],np.ones((m,1)))\n",
    "        Y_trans.append(yarray.reshape(-1))\n",
    "    Y_transform =  np.array(Y_trans).T\n",
    "    return Y_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = Y_transform(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inistialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(n_inputs, hidden_dims, output_dims=1, init_scale=0.05):\n",
    "    \"\"\"\n",
    "    Dynamically generate randomly initialized parameters for model based on user inputs.\n",
    "    Arguments:\n",
    "        n_inputs (int): number of inputs features\n",
    "        hidden_dims (list): number of neurons (integers) in each hidden layer\n",
    "        output_dims (int): number of output neurons in layer L\n",
    "        init_scale (float): scaling factor for random initialization \n",
    "    Returns:\n",
    "        parameters (dictionary): a dictionary of weight and bias parameters for each layer.\n",
    "    \"\"\"\n",
    "    layer_dims = [n_inputs]+hidden_dims+[output_dims]\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    parameters = {}\n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l-1],layer_dims[l])*init_scale\n",
    "        parameters['b'+str(l)] = np.zeros((1,layer_dims[l]))\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (4, 32)\n",
      "b1 (1, 32)\n",
      "W2 (32, 64)\n",
      "b2 (1, 64)\n",
      "W3 (64, 3)\n",
      "b3 (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out\n",
    "n_inputs = X_train.shape[1] # Extract number of inputs from training dat a\n",
    "hidden_layers = [32, 64] # Layer 1 will have 32 neurons, Layer 2 two wil l have 32 neurons\n",
    "n_outputs = 3\n",
    "parameters = init_params(n_inputs,hidden_layers,n_outputs)\n",
    "for key, value in parameters.items():\n",
    "    print(key,value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def softmax(z):\n",
    "    m = z.shape[1]\n",
    "    n = z.shape[0]\n",
    "    sm = np.zeros((z.shape))\n",
    "    maxvals = np.reshape(np.amax(z,1),(-1,n)).T.dot(np.ones((1,m)))\n",
    "    exps = np.exp(z - maxvals)    \n",
    "    expsums = np.sum(exps,axis=1)\n",
    "    for i in range(0,m): \n",
    "        sm[:,i] = np.divide(exps[:,i],expsums)\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[3],[4],[5]]).dot(np.array([[1,1,1]]))\n",
    "A = np.array([1,2,3])\n",
    "B = np.array([[1,2,4],[7,2,1]])\n",
    "np.reshape(np.amax(B,1),(-1,2)).T.dot(np.ones((1,3)))\n",
    "np.sum(B,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, tanh=False, print_shapes=False):\n",
    "    \"\"\"\n",
    "    Runs X inputs through the model layers (based on the number paramete\n",
    "rs in `paremters` dictionary) for a single forward pass\n",
    "    Arguments:\n",
    "        X (numpy array): input data to be run through the model. Input s\n",
    "ize should match `n_inputs` specified in W1 of parameters arg\n",
    "        parameters (dictionary): model weights and bias vectors for each\n",
    "layer\n",
    "        tanh (boolean): if True, use tanh activation for hidden layers,\n",
    " else use ReLU (default)\n",
    "        print_shapes (boolean): purely for demonstration purposes; if Tr\n",
    "ue, print layer shapes\n",
    "    Retruns:\n",
    "        A (numpy array): the final layer output for a single forward pas\n",
    "s (A[L])\n",
    "        caches (tuple): cached layer outputs (used for backprop) in the\n",
    " form of ((A_prev, W, b), Z)\n",
    "    \"\"\"\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        W = parameters['W'+str(l)] #read current weights\n",
    "        b = parameters['b'+str(l)]\n",
    "        A_prev = A                 #read previous A matrix\n",
    "        Z = A_prev.dot(W)+b        #current Z\n",
    "        \n",
    "        if print_shapes:\n",
    "            print(\"A[{}] shape:\".format(l), A_prev.shape) \n",
    "            print(\"W[{}] shape:\".format(l), W.shape) \n",
    "            print(\"Z[{}] shape:\".format(l), Z.shape)\n",
    "        \n",
    "        if l == L:\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            if tanh:\n",
    "                A = np.tanh(Z)\n",
    "            else:\n",
    "                A = relu(Z)\n",
    "        \n",
    "        cache = ((A_prev, W, b), Z)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    return A, caches\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A[1] shape: (150, 4)\n",
      "W[1] shape: (4, 32)\n",
      "Z[1] shape: (150, 32)\n",
      "A[2] shape: (150, 32)\n",
      "W[2] shape: (32, 64)\n",
      "Z[2] shape: (150, 64)\n",
      "A[3] shape: (150, 64)\n",
      "W[3] shape: (64, 3)\n",
      "Z[3] shape: (150, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it out\n",
    "n_inputs = X_train.shape[1] # Extract number of inputs from training data\n",
    "hidden_layers = [32, 64] # Layer 1 will have 32 neurons, Layer 2 two wil l have 64 neurons\n",
    "n_outputs = 3\n",
    "\n",
    "parameters = init_params(n_inputs,hidden_layers,n_outputs)\n",
    "# parameters = update_parameters(parameters,grads,0.01)\n",
    "AL, caches = L_model_forward(X_train,parameters,print_shapes=True) \n",
    "\n",
    "(a1,w2,b2), z2= caches[1]\n",
    "w2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the binary crossentropy loss\n",
    "    Arguments:\n",
    "        AL (numpy array): output of the last layer of the model (each el\n",
    "ement of array is sigmoid output between 0 and 1)\n",
    "        Y (numpy array): true Y values (0 or 1), should be same shape AL\n",
    "    Returns:\n",
    "        cost (float): the average loss over m instances\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    cost = -(1/m)*np.sum(np.multiply(Y,np.log(AL)))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0975392944192448"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it out\n",
    "# Calculate cost\n",
    "cost = compute_cost(AL,y_train)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax_backward(dA, cache):\n",
    "#     \"\"\"Calculate gradients of loss with respect to Z\"\"\" \n",
    "#     z = cache\n",
    "#     m = z.shape[1]\n",
    "#     sm_d = np.zeros((z.shape))\n",
    "#     exps = np.exp(z)\n",
    "#     expsums = np.sum(exps,axis=1)\n",
    "# #     print(exxps)\n",
    "#     for i in range(0,m): \n",
    "#         s = np.divide(exps[:,i],exps[:,i]+expsums-exps[:,i])\n",
    "#         sm_d[:,i] = np.multiply(s,1-s)\n",
    "#     dz = np.multiply(dA,sm_d)\n",
    "#     return dz\n",
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"Calculate gradients of loss with respect to Z\"\"\" \n",
    "    z = cache\n",
    "    n = z.shape[1]\n",
    "    s = softmax(z)\n",
    "    sm_d = np.zeros((z.shape))\n",
    "    for i in range(0,n):\n",
    "        s_in = s[:,i]\n",
    "        s_ex = np.delete(s,i,1)\n",
    "        dA_in = dA[:,i]\n",
    "        dA_ex = np.delete(dA,i,1)\n",
    "        dz_different = np.zeros((z.shape[0],z.shape[1]-1))\n",
    "        dz_same = np.multiply(s_in,1-s_in)\n",
    "        dz_different[:,0] = np.multiply(s_ex[:,0],-s_in)\n",
    "        dz_different[:,1] = np.multiply(s_ex[:,1],-s_in)\n",
    "#         print(dz_different.shape)\n",
    "        sm_d[:,i] = np.multiply(dA_in,dz_same)+np.multiply(dA_ex[:,0],dz_different[:,0])+np.multiply(dA_ex[:,1],dz_different[:,1])\n",
    "    return sm_d\n",
    "def tanh_backward(dA, cache):\n",
    "    \"\"\"Calculate gradients of loss with respect to Z\"\"\"\n",
    "    Z = cache\n",
    "    dZ = dA.T * (1 - np.tanh(Z)**2) # Multiply by dA (chain rule) return dZ\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"Calculate gradients of loss with respect to Z\"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA.T, copy=True) # Multiply by dA (chain rule) dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL,Y,caches,tanh=True):\n",
    "    grads = {}\n",
    "    L = len(caches) # number of layers\n",
    "    m = AL.shape[0] # number of observers\n",
    "    \n",
    "    (A_prev, W, b), Z = caches[-1]\n",
    "    \n",
    "    dAL = -np.divide(Y,AL) # partial J/partial A\n",
    "    dZ = softmax_backward(dAL,Z) #partial J/partial Z\n",
    "    \n",
    "    dW = (1./m)*np.dot(dZ.T, A_prev) # partial J/partial W\n",
    "    db = (1./m)*np.sum(dZ.T, axis=1, keepdims=True) # partial J/partial b\n",
    "    \n",
    "    dA_prev = np.dot(W, dZ.T)\n",
    "    \n",
    "    grads['dW'+str(L)] = dW\n",
    "    grads['db'+str(L)] = db\n",
    "    \n",
    "    for l in reversed(range(1,L)):\n",
    "        (A_prev, W, b), Z = caches[l-1]\n",
    "        if tanh:\n",
    "            dZ = tanh_backward(dA_prev,Z)\n",
    "        else:\n",
    "            dZ = relu_backward(dA_prev,Z)\n",
    "        dW = (1. / m) * np.dot(dZ.T, A_prev)\n",
    "        db = (1. / m) * np.sum(dZ.T, axis=1, keepdims=True) # Calculate dA_prev for layer L-1\n",
    "        dA_prev = np.dot(W, dZ.T)\n",
    "        # Store gradients\n",
    "        grads[\"dW\" + str(l)] = dW\n",
    "        grads[\"db\" + str(l)] = db\n",
    "        \n",
    "    return grads\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = L_model_backward(AL, y_train, caches, tanh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['db1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters W/b with gradients dW/db for each layer, to compl\n",
    "ete a step of backpropagation\n",
    "    Arguments:\n",
    "        parameters (dictionary): model weights and bias vectors for each\n",
    "layer\n",
    "        grads (dictionary): parameter gradients (output of `L_model_back\n",
    "ward` function)\n",
    "        learning_rate (float): learning rate for gradient descent step\n",
    "    Returns:\n",
    "        parameters (dictionary): updated weight and bias parameters\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network # Update rule for each parameter; loop through each layer\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)].T # Tranpose to match weight shape\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)].T # Tranpose to match weight shape\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = update_parameters(parameters,grads,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tCost: 1.0984875263160834\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set parameters\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = 3\n",
    "hidden_layers = [32,64] # 2 layers, each with 32 neurons\n",
    "init_scale = 0.02\n",
    "learning_rate = 0.1\n",
    "tanh = True\n",
    "\n",
    "# Randomly initialize params\n",
    "parameters = init_params(n_inputs,hidden_layers, n_outputs, init_scale=init_scale)\n",
    "\n",
    "# Run model\n",
    "n_epochs =2000\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Run forward propogation\n",
    "    AL, caches = L_model_forward(X_train, parameters, tanh=tanh)\n",
    "    \n",
    "    # Compute cost/accuracy; print after 2000 epochs\n",
    "    loss = compute_cost(AL, y_train)\n",
    "    losses.append(loss)\n",
    "#     if epoch % (n_epochs//10) == 0:\n",
    "    if epoch % 2000 == 0:\n",
    "#         y_hat = np.where(AL>0.5,1,0).reshape(-1)\n",
    "        print(\"Epoch:\",epoch, \"\\tCost:\", loss)\n",
    "\n",
    "    # Run backward popogation and update parameters\n",
    "    grads = L_model_backward(AL, y_train, caches, tanh=tanh)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "AL, caches = L_model_forward(X_train, parameters, tanh=tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL.shape\n",
    "AL_max_index = np.argmax(AL, axis=1)\n",
    "AL_max_index\n",
    "accuracy_score(y_train_original,AL_max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [4, 6]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2,3],[4,5,6]])\n",
    "a_del = np.delete(A, 1, 1)\n",
    "a_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parameters)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
